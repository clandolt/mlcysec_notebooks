

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tutorial 2.2: Deep Learning based IDS &mdash; CISPA Machine Learning in Cybersecurity v0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=d2d258e8" />
      <link rel="stylesheet" type="text/css" href="../../_static/nbsphinx-code-cells.css?v=2aa19091" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=82d01d63" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=34cd777e"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Tutorial 2.3: Analyzing Application-Layer Protocols" href="../tutorial2_3_analyzing_application-layer_protocols/tutorial2_3_analyzing_application-layer_protocols.html" />
    <link rel="prev" title="Tutorial 2.1: Intrusion Detection System" href="../tutorial2_anomaly_detection/tutorial2_anomaly_detection.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            CISPA Machine Learning in Cybersecurity
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Tutorial 1: Getting started:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_started_with_jupyter_and_python/getting_started_with_jupyter_and_python.html">Getting started 1: Working with Jupyter and Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../discover_visualize_gain_insights/discover_visualize_gain_insights.html">Getting Started 2: How to Load and Visualize Data for Cyber Threat Intelligence Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started_with_ml/getting_started_with_ml.html">Getting Started 3: Classic Machine Learning for Cybersecurity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started_with_deep_learning/getting_started_with_deep_learning.html">Getting Started 4: Deep Learning for Cybersecurity</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorial 2: Intrusion Detection:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../tutorial2_anomaly_detection/tutorial2_anomaly_detection.html">Tutorial 2.1: Intrusion Detection System</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tutorial 2.2: Deep Learning based IDS</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Tutorial-Objectives">Tutorial Objectives</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Neural-Network-Based-Anomaly-Detection-in-Cybersecurity">Neural Network-Based Anomaly Detection in Cybersecurity</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Variational-Autoencoder-(VAE)">Variational Autoencoder (VAE)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Probabilistic-Model">Probabilistic Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Training-Objective:-Evidence-Lower-Bound-(ELBO)">Training Objective: Evidence Lower Bound (ELBO)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Intuition">Intuition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Anomaly-Detection-with-VAE">Anomaly Detection with VAE</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Step-1:-Load-and-Explore-the-KDDCUP99-Dataset">Step 1: Load and Explore the KDDCUP99 Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Step-2:-Data-Preprocessing">Step 2: Data Preprocessing</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Step-3:-Train-Test-Split">Step 3: Train-Test Split</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Step-4:-Variational-Autoencoder-(VAE)-Implementation">Step 4: Variational Autoencoder (VAE) Implementation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#One-Class-Neural-Network-(OC-NN)">One-Class Neural Network (OC-NN)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Mathematical-Formulation">Mathematical Formulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">Intuition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Anomaly-Scoring">Anomaly Scoring</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial2_3_analyzing_application-layer_protocols/tutorial2_3_analyzing_application-layer_protocols.html">Tutorial 2.3: Analyzing Application-Layer Protocols</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">CISPA Machine Learning in Cybersecurity</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Tutorial 2.2: Deep Learning based IDS</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorial_notebooks/tutorial2_2_deep_learning_anomaly_detection/tutorial2_2_deep_learning_anomaly_detection.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Tutorial-2.2:-Deep-Learning-based-IDS">
<h1>Tutorial 2.2: Deep Learning based IDS<a class="headerlink" href="#Tutorial-2.2:-Deep-Learning-based-IDS" title="Link to this heading"></a></h1>
<img alt="Status" src="https://img.shields.io/static/v1.svg?label=Status&amp;message=Under_Construction&amp;color=orange" />
<div class="line-block">
<div class="line"><strong>Open notebook on:</strong> <a class="reference external" href="https://github.com/clandolt/mlcysec_notebooks/blob/main/source/tutorial_notebooks/tutorial2_anomaly_detection/tutorial2_anomaly_detection.ipynb"><img alt="View filled on Github" src="https://img.shields.io/static/v1.svg?logo=github&amp;label=Repo&amp;message=View%20On%20Github&amp;color=lightgrey" /></a> <a class="reference external" href="https://colab.research.google.com/github/clandolt/mlcysec_notebooks/blob/main/source/tutorial_notebooks/tutorial2_anomaly_detection/tutorial2_anomaly_detection.ipynb"><img alt="Open filled In Collab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></div>
<div class="line"><strong>Author:</strong> Christoph R. Landolt</div>
</div>
<p>In modern cybersecurity, labeled attack data is often scarce or incomplete. Many attacks are unknown, rare, or stealthy (APT, ATA). Neural network-based unsupervised models, such as Variational Autoencoders (VAE) and One-Class Neural Networks (OC-NN), can learn the normal behavior of network traffic and flag deviations as anomalies. These approaches are particularly useful for intrusion detection, malware monitoring, and unusual user behavior detection.</p>
<section id="Tutorial-Objectives">
<h2>Tutorial Objectives<a class="headerlink" href="#Tutorial-Objectives" title="Link to this heading"></a></h2>
<p>By the end of this tutorial, you will be able to:</p>
<ul class="simple">
<li><p>Understand neural network approaches to anomaly detection.</p></li>
<li><p>Explain the mathematical formulation of VAE and OC-NN for anomaly detection.</p></li>
<li><p>Implement VAE and OC-NN using PyTorch.</p></li>
<li><p>Evaluate anomaly detection performance on the KDDCUP99 dataset.</p></li>
</ul>
</section>
<section id="Neural-Network-Based-Anomaly-Detection-in-Cybersecurity">
<h2>Neural Network-Based Anomaly Detection in Cybersecurity<a class="headerlink" href="#Neural-Network-Based-Anomaly-Detection-in-Cybersecurity" title="Link to this heading"></a></h2>
<p>Traditional anomaly detection algorithms, such as Isolation Forest or One-Class SVM, are effective for tabular data, but they can struggle with high-dimensional, complex, or non-linear feature distributions. Neural networks provide:</p>
<ul class="simple">
<li><p><strong>Flexibility:</strong> Can model complex, non-linear relationships.</p></li>
<li><p><strong>Feature learning:</strong> Automatically extract latent representations.</p></li>
<li><p><strong>Probabilistic modeling:</strong> In the case of VAE, estimate the likelihood of each observation.</p></li>
</ul>
</section>
<section id="Variational-Autoencoder-(VAE)">
<h2>Variational Autoencoder (VAE)<a class="headerlink" href="#Variational-Autoencoder-(VAE)" title="Link to this heading"></a></h2>
<div class="line-block">
<div class="line">A <strong>Variational Autoencoder (VAE)</strong> is a <em>generative probabilistic model</em> that learns a continuous latent representation of data.</div>
<div class="line">It assumes that each observation <span class="math notranslate nohighlight">\(x\)</span> is generated from a latent variable <span class="math notranslate nohighlight">\(z\)</span> through a conditional distribution <span class="math notranslate nohighlight">\(p_\theta(x \mid z)\)</span>.</div>
</div>
<p><img alt="VAE" class="no-scaled-link" src="../../_images/VAE.jpg" style="width: 600px;" /></p>
<section id="Probabilistic-Model">
<h3>Probabilistic Model<a class="headerlink" href="#Probabilistic-Model" title="Link to this heading"></a></h3>
<p>The data likelihood is defined as:</p>
<div class="math notranslate nohighlight">
\[p_\theta(x) = \int p_\theta(x \mid z) \, p(z) \, dz\]</div>
<p>where the prior over latent variables is a standard normal distribution:</p>
<div class="math notranslate nohighlight">
\[p(z) = \mathcal{N}(0, I)\]</div>
<p>Since the true posterior <span class="math notranslate nohighlight">\(p_\theta(z \mid x)\)</span> is intractable, the VAE introduces an approximate posterior (encoder) <span class="math notranslate nohighlight">\(q_\phi(z \mid x)\)</span> to estimate it.</p>
</section>
<section id="Training-Objective:-Evidence-Lower-Bound-(ELBO)">
<h3>Training Objective: Evidence Lower Bound (ELBO)<a class="headerlink" href="#Training-Objective:-Evidence-Lower-Bound-(ELBO)" title="Link to this heading"></a></h3>
<p>The model is trained by maximizing the <strong>Evidence Lower Bound (ELBO)</strong>:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\theta, \phi; x)
=
\mathbb{E}_{q_\phi(z \mid x)} [ \log p_\theta(x \mid z) ]
-
D_{\mathrm{KL}}(q_\phi(z \mid x) \, \| \, p(z))\]</div>
<ul class="simple">
<li><p>The <strong>first term</strong> is the <em>reconstruction likelihood</em>, encouraging accurate reconstruction of inputs.</p></li>
<li><p>The <strong>second term</strong> is the <em>Kullback–Leibler (KL) divergence</em>, acting as a regularizer to keep the latent space close to the prior <span class="math notranslate nohighlight">\(\mathcal{N}(0, I)\)</span>.</p></li>
</ul>
</section>
<section id="Intuition">
<h3>Intuition<a class="headerlink" href="#Intuition" title="Link to this heading"></a></h3>
<p>The VAE consists of two main components:</p>
<ul>
<li><div class="line-block">
<div class="line"><strong>Encoder (Inference network)</strong>: learns to map data <span class="math notranslate nohighlight">\(x\)</span> into a latent representation <span class="math notranslate nohighlight">\(z\)</span></div>
<div class="line"><span class="math notranslate nohighlight">\(q_\phi(z \mid x) = \mathcal{N}(\mu(x), \sigma^2(x) I)\)</span></div>
</div>
</li>
<li><div class="line-block">
<div class="line"><strong>Decoder (Generative network)</strong>: reconstructs data from the latent representation</div>
<div class="line"><span class="math notranslate nohighlight">\(p_\theta(x \mid z) = \mathcal{N}(f_\theta(z), \sigma^2 I)\)</span></div>
</div>
</li>
</ul>
</section>
<section id="Anomaly-Detection-with-VAE">
<h3>Anomaly Detection with VAE<a class="headerlink" href="#Anomaly-Detection-with-VAE" title="Link to this heading"></a></h3>
<div class="line-block">
<div class="line">For anomaly detection, the VAE is trained <strong>only on normal samples</strong> to capture the manifold of legitimate data.</div>
<div class="line">At test time, we compute the <strong>reconstruction error</strong> for each input:</div>
</div>
<div class="math notranslate nohighlight">
\[\text{Error}(x) = \| x - \hat{x} \|^2\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{x} = f_\theta(z)\)</span> is the reconstructed input.</p>
<ul class="simple">
<li><p><strong>Low reconstruction error</strong> → sample lies on the learned manifold → likely normal</p></li>
<li><p><strong>High reconstruction error</strong> → sample deviates from normal behavior → potential anomaly</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[34]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Importing required libraries</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">IsolationForest</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">OneClassSVM</span>
<span class="kn">from</span> <span class="nn">sklearn.covariance</span> <span class="kn">import</span> <span class="n">EllipticEnvelope</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">resample</span>
<br/><br/></pre></div>
</div>
</div>
<section id="Step-1:-Load-and-Explore-the-KDDCUP99-Dataset">
<h4>Step 1: Load and Explore the KDDCUP99 Dataset<a class="headerlink" href="#Step-1:-Load-and-Explore-the-KDDCUP99-Dataset" title="Link to this heading"></a></h4>
<p>First, we’ll load the <code class="docutils literal notranslate"><span class="pre">SA</span></code> subset of the <strong>KDDCUP99 dataset</strong> to keep computation manageable. Then we’ll explore and visualize the data.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[35]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><br/><span></span><span class="c1"># ### Step 1: Load and Explore the KDDCUP99 Dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">fetch_kddcup99</span><span class="p">(</span>
    <span class="n">subset</span><span class="o">=</span><span class="s2">&quot;SA&quot;</span><span class="p">,</span>             <span class="c1"># Use the &#39;SA&#39; subset (smaller sample)</span>
    <span class="n">percent10</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>          <span class="c1"># Use 10% of the full dataset for efficiency</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>         <span class="c1"># Ensure reproducibility</span>
    <span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>         <span class="c1"># Return data and labels separately</span>
    <span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span>            <span class="c1"># Load as pandas DataFrame</span>
<span class="p">)</span>

<span class="c1"># Convert binary label: 1 = attack, 0 = normal</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">!=</span> <span class="sa">b</span><span class="s2">&quot;normal.&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

<span class="c1"># Take only 10% of the data for quick demonstration</span>
<span class="n">X</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Display dataset stats</span>
<span class="n">n_samples</span><span class="p">,</span> <span class="n">anomaly_frac</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">n_samples</span><span class="si">}</span><span class="s2"> datapoints with </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">}</span><span class="s2"> anomalies (</span><span class="si">{</span><span class="n">anomaly_frac</span><span class="si">:</span><span class="s2">.02%</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Plot label distribution</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Normal&#39;</span><span class="p">,</span> <span class="s1">&#39;Attack&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Histogram of Labels&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Label&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
10065 datapoints with 338 anomalies (3.36%)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorial_notebooks_tutorial2_2_deep_learning_anomaly_detection_tutorial2_2_deep_learning_anomaly_detection_4_1.png" src="../../_images/tutorial_notebooks_tutorial2_2_deep_learning_anomaly_detection_tutorial2_2_deep_learning_anomaly_detection_4_1.png" />
</div>
</div>
<p><strong>Notes:</strong></p>
<ul class="simple">
<li><p>The histogram provides a visual overview of <strong>class imbalance</strong> in the dataset. In the KDDCUP99 subset, normal traffic far outnumbers attack events.</p></li>
<li><p>This imbalance is <strong>typical in cybersecurity datasets</strong>, reflecting real-world conditions where attacks are rare relative to benign activity.</p></li>
<li><p>From a theoretical perspective, Intrusion Detection Systems (IDS) face two main challenges in such imbalanced environments:</p>
<ol class="arabic simple">
<li><p><strong>Scarcity of labeled attack data</strong>: Many attack patterns are unknown, costly to label, or represent vulnerabilities not yet exploited.</p></li>
<li><p><strong>Diversity of attack types</strong>: Attacks can range from common automated probes to sophisticated Advanced Persistent Threats (APT) and Advanced Targeted Attacks (ATA), which occur rarely and blend into normal traffic.</p></li>
</ol>
</li>
<li><p>Therefore, the observed class imbalance in the histogram <strong>justifies the use of unsupervised anomaly detection models</strong> (such as Isolation Forest), which do not rely on balanced labeled datasets but exploit the intrinsic structure of the data to detect deviations.</p></li>
</ul>
</section>
<section id="Step-2:-Data-Preprocessing">
<h4>Step 2: Data Preprocessing<a class="headerlink" href="#Step-2:-Data-Preprocessing" title="Link to this heading"></a></h4>
<p>Before training, categorical (non-numeric) features must be converted into numerical form. We’ll use <strong>one-hot encoding</strong> with <code class="docutils literal notranslate"><span class="pre">pandas.get_dummies()</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[36]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert categorical variables to numerical format</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Feature matrix shape after encoding: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">X</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Feature matrix shape after encoding: (10065, 6536)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[36]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>duration_0</th>
      <th>duration_1</th>
      <th>duration_2</th>
      <th>duration_3</th>
      <th>duration_4</th>
      <th>duration_5</th>
      <th>duration_6</th>
      <th>duration_7</th>
      <th>duration_8</th>
      <th>duration_9</th>
      <th>...</th>
      <th>dst_host_srv_rerror_rate_0.91</th>
      <th>dst_host_srv_rerror_rate_0.92</th>
      <th>dst_host_srv_rerror_rate_0.93</th>
      <th>dst_host_srv_rerror_rate_0.94</th>
      <th>dst_host_srv_rerror_rate_0.95</th>
      <th>dst_host_srv_rerror_rate_0.96</th>
      <th>dst_host_srv_rerror_rate_0.97</th>
      <th>dst_host_srv_rerror_rate_0.98</th>
      <th>dst_host_srv_rerror_rate_0.99</th>
      <th>dst_host_srv_rerror_rate_1.0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>26890</th>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>35471</th>
      <td>False</td>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>37027</th>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>80164</th>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>73649</th>
      <td>True</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>...</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 6536 columns</p>
</div></div>
</div>
<p><strong>Notes:</strong></p>
<ul class="simple">
<li><p>Many columns in KDDCUP99 are categorical (e.g., protocol type, service, flag).</p></li>
<li><p>One-hot encoding converts these categories into binary vectors, making them compatible with ML models.</p></li>
</ul>
</section>
</section>
<section id="Step-3:-Train-Test-Split">
<h3>Step 3: Train-Test Split<a class="headerlink" href="#Step-3:-Train-Test-Split" title="Link to this heading"></a></h3>
<p>We split the dataset into <strong>training (80%)</strong> and <strong>testing (20%)</strong> subsets.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[37]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="c1"># Keep only normal samples in the training set</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training only on normal points: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="si">}</span><span class="s2"> samples&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing samples:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Training only on normal points: 7784 samples
Testing samples: 2013
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[38]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">classification_report</span>

<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>

<span class="c1"># --- Convert Data ---</span>
<span class="n">X_train_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">X_test_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<section id="Step-4:-Variational-Autoencoder-(VAE)-Implementation">
<h4>Step 4: Variational Autoencoder (VAE) Implementation<a class="headerlink" href="#Step-4:-Variational-Autoencoder-(VAE)-Implementation" title="Link to this heading"></a></h4>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[39]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># --- 1. Encoder Class ---</span>
<span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Feature extraction part (equivalent to fc1 + ReLU in original)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="c1"># Layers for mean (mu) and log variance (logvar)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_mu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_logvar</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_mu</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_logvar</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span>

<span class="c1"># --- 2. Decoder Class ---</span>
<span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Decoder network (equivalent to fc2 + ReLU + fc3 in original)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
            <span class="c1"># A final activation (like Sigmoid or Tanh) is often added here</span>
            <span class="c1"># for image/bounded data, but is omitted for general real-valued data</span>
            <span class="c1"># where the loss is MSE (as per original code).</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_net</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>


<span class="c1"># --- 3. Adapted VAE Class (Main Model) ---</span>
<span class="k">class</span> <span class="nc">VAE</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Instantiate the explicit Encoder and Decoder modules</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reparameterize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Samples z from the latent distribution (N(mu, exp(logvar)))</span>
<span class="sd">        using the reparameterization trick.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">logvar</span><span class="p">)</span>
        <span class="c1"># eps is a random vector from the standard normal distribution</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">std</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># 1. Encode: Get the parameters of the latent distribution</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># 2. Reparameterize: Sample a latent vector z</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reparameterize</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>

        <span class="c1"># 3. Decode: Reconstruct the input</span>
        <span class="n">recon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">recon</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span>

<span class="c1"># --- 4. Loss Function ---</span>
<span class="k">def</span> <span class="nf">vae_loss</span><span class="p">(</span><span class="n">recon_x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the VAE loss, which is the sum of:</span>
<span class="sd">    1. Reconstruction Loss (e.g., MSE or BCE)</span>
<span class="sd">    2. KL Divergence Loss</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Reconstruction Loss (using MSE as per original code)</span>
    <span class="n">recon_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)(</span><span class="n">recon_x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="c1"># Using &#39;sum&#39; instead of &#39;mean&#39; for better KLD scaling, then we&#39;ll divide by batch size/N below</span>

    <span class="c1"># KL Divergence Loss: KLD = -0.5 * sum(1 + logvar - mu^2 - exp(logvar))</span>
    <span class="n">kld</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">logvar</span> <span class="o">-</span> <span class="n">mu</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">logvar</span><span class="o">.</span><span class="n">exp</span><span class="p">())</span>

    <span class="c1"># Total loss (divided by batch size for consistency)</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">recon_loss</span> <span class="o">+</span> <span class="n">kld</span><span class="p">)</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># x.size(0) is the batch size</span>
    <span class="k">return</span> <span class="n">total_loss</span>

<span class="c1"># --- 5. Model Instantiation and Optimizer ---</span>
<span class="n">vae</span> <span class="o">=</span> <span class="n">VAE</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">vae</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>

<span class="c1"># print neural network architecture</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vae</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
VAE(
  (encoder): Encoder(
    (feature_extractor): Sequential(
      (0): Linear(in_features=6536, out_features=64, bias=True)
      (1): Tanh()
    )
    (fc_mu): Linear(in_features=64, out_features=10, bias=True)
    (fc_logvar): Linear(in_features=64, out_features=10, bias=True)
  )
  (decoder): Decoder(
    (decoder_net): Sequential(
      (0): Linear(in_features=10, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=6536, bias=True)
    )
  )
)
</pre></div></div>
</div>
<p>Training</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[40]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="c1"># --- Track losses ---</span>
<span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_losses</span> <span class="o">=</span> <span class="p">[]</span>


<span class="c1"># --- 6. Training Loop ---</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">vae</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># Forward pass</span>
    <span class="n">recon_x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">vae</span><span class="p">(</span><span class="n">X_train_tensor</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">vae_loss</span><span class="p">(</span><span class="n">recon_x</span><span class="p">,</span> <span class="n">X_train_tensor</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Evaluate on test data (reconstruction only)</span>
    <span class="n">vae</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">recon_test</span><span class="p">,</span> <span class="n">mu_t</span><span class="p">,</span> <span class="n">logvar_t</span> <span class="o">=</span> <span class="n">vae</span><span class="p">(</span><span class="n">X_test_tensor</span><span class="p">)</span>
        <span class="n">test_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()(</span><span class="n">recon_test</span><span class="p">,</span> <span class="n">X_test_tensor</span><span class="p">)</span>

    <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">test_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">:</span><span class="s2">02d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s2">, Train Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Test Loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># --- Plot train and test reconstruction loss ---</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Train Reconstruction Loss&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">test_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Test Reconstruction Loss&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;s&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean Squared Error (MSE) Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;VAE Training and Test Reconstruction Loss over Epochs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Epoch 01/50, Train Loss: 579.9401, Test Loss: 0.0814
Epoch 05/50, Train Loss: 403.6346, Test Loss: 0.0561
Epoch 10/50, Train Loss: 245.9321, Test Loss: 0.0337
Epoch 15/50, Train Loss: 147.6533, Test Loss: 0.0201
Epoch 20/50, Train Loss: 90.6288, Test Loss: 0.0121
Epoch 25/50, Train Loss: 60.2150, Test Loss: 0.0079
Epoch 30/50, Train Loss: 45.1999, Test Loss: 0.0056
Epoch 35/50, Train Loss: 36.8219, Test Loss: 0.0044
Epoch 40/50, Train Loss: 31.4613, Test Loss: 0.0034
Epoch 45/50, Train Loss: 28.2044, Test Loss: 0.0029
Epoch 50/50, Train Loss: 26.5779, Test Loss: 0.0026
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorial_notebooks_tutorial2_2_deep_learning_anomaly_detection_tutorial2_2_deep_learning_anomaly_detection_15_1.png" src="../../_images/tutorial_notebooks_tutorial2_2_deep_learning_anomaly_detection_tutorial2_2_deep_learning_anomaly_detection_15_1.png" />
</div>
</div>
<p>Compute reconstruction-based anomaly scores: High reconstruction error → likely anomaly.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[41]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># --- Compute Reconstruction Error on Test Set ---</span>
<span class="n">vae</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">recon_test</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">vae</span><span class="p">(</span><span class="n">X_test_tensor</span><span class="p">)</span>
<span class="n">errors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">X_test_tensor</span> <span class="o">-</span> <span class="n">recon_test</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="c1"># --- Choose Threshold ---</span>
<span class="c1"># Simple heuristic: threshold = 95th percentile of reconstruction error on training set</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">recon_train</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">vae</span><span class="p">(</span><span class="n">X_train_tensor</span><span class="p">)</span>
<span class="n">train_errors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">X_train_tensor</span> <span class="o">-</span> <span class="n">recon_train</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">train_errors</span><span class="p">,</span> <span class="mi">95</span><span class="p">)</span>

<span class="c1"># --- Predict anomalies ---</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">errors</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="c1"># --- Confusion Matrix ---</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">cm</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Normal&#39;</span><span class="p">,</span> <span class="s1">&#39;Attack&#39;</span><span class="p">]))</span>

<span class="c1"># --- Plot Confusion Matrix ---</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Normal&#39;</span><span class="p">,</span> <span class="s1">&#39;Attack&#39;</span><span class="p">],</span> <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Normal&#39;</span><span class="p">,</span> <span class="s1">&#39;Attack&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted Label&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Label&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Confusion Matrix for VAE-based Anomaly Detection&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Confusion Matrix:
 [[1825  118]
 [  53   17]]

Classification Report:
               precision    recall  f1-score   support

      Normal       0.97      0.94      0.96      1943
      Attack       0.13      0.24      0.17        70

    accuracy                           0.92      2013
   macro avg       0.55      0.59      0.56      2013
weighted avg       0.94      0.92      0.93      2013

</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorial_notebooks_tutorial2_2_deep_learning_anomaly_detection_tutorial2_2_deep_learning_anomaly_detection_17_1.png" src="../../_images/tutorial_notebooks_tutorial2_2_deep_learning_anomaly_detection_tutorial2_2_deep_learning_anomaly_detection_17_1.png" />
</div>
</div>
</section>
</section>
</section>
<section id="One-Class-Neural-Network-(OC-NN)">
<h2>One-Class Neural Network (OC-NN)<a class="headerlink" href="#One-Class-Neural-Network-(OC-NN)" title="Link to this heading"></a></h2>
<div class="line-block">
<div class="line">The <strong>One-Class Neural Network (OC-NN)</strong> is a neural extension of the traditional <strong>One-Class SVM (OC-SVM)</strong>.</div>
<div class="line">Its goal is to learn a compact representation of <em>normal</em> data in a latent feature space, effectively separating it from potential anomalies.</div>
</div>
<p><strong>Idea:</strong> Instead of relying on a predefined kernel as in OC-SVM, OC-NN uses a neural network <span class="math notranslate nohighlight">\(f_\theta(x)\)</span> to <strong>map input data into a latent space</strong> where normal samples are concentrated around the origin (or a compact region).</p>
<section id="Mathematical-Formulation">
<h3>Mathematical Formulation<a class="headerlink" href="#Mathematical-Formulation" title="Link to this heading"></a></h3>
<div class="line-block">
<div class="line">Let <span class="math notranslate nohighlight">\(f_\theta(x)\)</span> denote the neural network output for input <span class="math notranslate nohighlight">\(x\)</span>.</div>
<div class="line">The OC-NN minimizes the distance of normal samples from a learned boundary (radius <span class="math notranslate nohighlight">\(r\)</span>) while allowing a small fraction <span class="math notranslate nohighlight">\(\nu\)</span> of samples to fall outside:</div>
</div>
<div class="math notranslate nohighlight">
\[\min_\theta \; \frac{1}{n} \sum_{i=1}^n \max(0, f_\theta(x_i) - r)\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(r\)</span> is the <em>radius</em> (determined from the <span class="math notranslate nohighlight">\(\nu\)</span>-th quantile of outputs)</p></li>
<li><p><span class="math notranslate nohighlight">\(\nu \in (0, 1)\)</span> approximates the <em>expected fraction of anomalies</em></p></li>
</ul>
</section>
<section id="id1">
<h3>Intuition<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Normal samples:</strong> mapped close to the origin (or within the radius <span class="math notranslate nohighlight">\(r\)</span>)</p></li>
<li><p><strong>Anomalous samples:</strong> lie outside this compact region, with higher output scores</p></li>
</ul>
<p>Thus, the network learns a <strong>boundary around normal data</strong>, similar to the hypersphere used in One-Class SVM.</p>
</section>
<section id="Anomaly-Scoring">
<h3>Anomaly Scoring<a class="headerlink" href="#Anomaly-Scoring" title="Link to this heading"></a></h3>
<p>After training, the <strong>anomaly score</strong> for a new input <span class="math notranslate nohighlight">\(x\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[s(x) = f_\theta(x) - r\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(s(x) \leq 0\)</span> → sample lies inside the normal region → <strong>normal</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(s(x) &gt; 0\)</span> → sample lies outside the region → <strong>anomalous</strong></p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[42]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">OCNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">ocnn_loss</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="n">nu</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">dist</span><span class="p">),</span> <span class="n">dist</span> <span class="o">-</span> <span class="n">r</span><span class="p">))</span>

<span class="n">ocnn</span> <span class="o">=</span> <span class="n">OCNN</span><span class="p">(</span><span class="n">input_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">ocnn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>

<span class="c1"># print neural network architecture</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ocnn</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
OCNN(
  (fc1): Linear(in_features=6536, out_features=32, bias=True)
  (fc2): Linear(in_features=32, out_features=32, bias=True)
  (fc3): Linear(in_features=32, out_features=1, bias=True)
)
</pre></div></div>
</div>
<p>Training</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">nu</span> <span class="o">=</span> <span class="mf">0.04</span>  <span class="c1"># expected anomaly fraction</span>


<span class="c1"># --- Track Losses ---</span>
<span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># --- Training Loop ---</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">ocnn</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">ocnn</span><span class="p">(</span><span class="n">X_train_tensor</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">ocnn_loss</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">nu</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Compute test loss for monitoring (using same loss function)</span>
    <span class="n">ocnn</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">test_out</span> <span class="o">=</span> <span class="n">ocnn</span><span class="p">(</span><span class="n">X_test_tensor</span><span class="p">)</span>
        <span class="n">test_loss</span> <span class="o">=</span> <span class="n">ocnn_loss</span><span class="p">(</span><span class="n">test_out</span><span class="p">,</span> <span class="n">nu</span><span class="p">)</span>

    <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">test_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">:</span><span class="s2">02d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s2">, Train Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Test Loss: </span><span class="si">{</span><span class="n">test_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># --- Plot Train vs Test Loss ---</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Train Loss&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">test_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Test Loss&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;s&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;OC-NN Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;One-Class Neural Network (OC-NN) Training and Test Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Epoch 01/50, Train Loss: 0.0037, Test Loss: 0.0040
Epoch 05/50, Train Loss: 0.0041, Test Loss: 0.0031
Epoch 10/50, Train Loss: 0.0019, Test Loss: 0.0015
Epoch 15/50, Train Loss: 0.0010, Test Loss: 0.0009
Epoch 20/50, Train Loss: 0.0009, Test Loss: 0.0009
Epoch 25/50, Train Loss: 0.0006, Test Loss: 0.0006
Epoch 30/50, Train Loss: 0.0006, Test Loss: 0.0005
Epoch 35/50, Train Loss: 0.0006, Test Loss: 0.0006
Epoch 40/50, Train Loss: 0.0005, Test Loss: 0.0005
Epoch 45/50, Train Loss: 0.0004, Test Loss: 0.0004
Epoch 50/50, Train Loss: 0.0002, Test Loss: 0.0002
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorial_notebooks_tutorial2_2_deep_learning_anomaly_detection_tutorial2_2_deep_learning_anomaly_detection_21_1.png" src="../../_images/tutorial_notebooks_tutorial2_2_deep_learning_anomaly_detection_tutorial2_2_deep_learning_anomaly_detection_21_1.png" />
</div>
</div>
<p>Anomaly Scores</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[44]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># --- Compute Anomaly Scores ---</span>
<span class="n">ocnn</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">train_scores</span> <span class="o">=</span> <span class="n">ocnn</span><span class="p">(</span><span class="n">X_train_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">test_scores</span> <span class="o">=</span> <span class="n">ocnn</span><span class="p">(</span><span class="n">X_test_tensor</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="c1"># --- Determine Radius Threshold ---</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">train_scores</span><span class="p">,</span> <span class="mi">95</span><span class="p">)</span>  <span class="c1"># 95th percentile</span>

<span class="c1"># --- Predict Anomalies ---</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">test_scores</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="c1"># --- Confusion Matrix and Metrics ---</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Confusion Matrix:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">cm</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Normal&#39;</span><span class="p">,</span> <span class="s1">&#39;Attack&#39;</span><span class="p">]))</span>

<span class="c1"># --- Plot Confusion Matrix ---</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">,</span>
            <span class="n">xticklabels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Normal&#39;</span><span class="p">,</span> <span class="s1">&#39;Attack&#39;</span><span class="p">],</span> <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Normal&#39;</span><span class="p">,</span> <span class="s1">&#39;Attack&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted Label&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Label&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Confusion Matrix for OC-NN Anomaly Detection&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

Confusion Matrix:
 [[1799  144]
 [  70    0]]

Classification Report:
               precision    recall  f1-score   support

      Normal       0.96      0.93      0.94      1943
      Attack       0.00      0.00      0.00        70

    accuracy                           0.89      2013
   macro avg       0.48      0.46      0.47      2013
weighted avg       0.93      0.89      0.91      2013

</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorial_notebooks_tutorial2_2_deep_learning_anomaly_detection_tutorial2_2_deep_learning_anomaly_detection_23_1.png" src="../../_images/tutorial_notebooks_tutorial2_2_deep_learning_anomaly_detection_tutorial2_2_deep_learning_anomaly_detection_23_1.png" />
</div>
</div>
</section>
</section>
<section id="Conclusion">
<h2>Conclusion<a class="headerlink" href="#Conclusion" title="Link to this heading"></a></h2>
<hr class="docutils" />
<div class="line-block">
<div class="line"><a class="reference external" href="https://github.com/clandolt/mlcysec_notebooks/"><img alt="Star our repository" src="https://img.shields.io/static/v1.svg?logo=star&amp;label=⭐&amp;message=Star%20Our%20Repository&amp;color=yellow" /></a> If you found this tutorial helpful, please <strong>⭐ star our repository</strong> to show your support.</div>
<div class="line"><a class="reference external" href="https://github.com/clandolt/mlcysec_notebooks/issues"><img alt="Ask questions" src="https://img.shields.io/static/v1.svg?logo=star&amp;label=❔&amp;message=Ask%20Questions&amp;color=9cf" /></a> For any <strong>questions</strong>, <strong>typos</strong>, or <strong>bugs</strong>, kindly open an issue on GitHub — we appreciate your feedback!</div>
</div>
<hr class="docutils" />
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../tutorial2_anomaly_detection/tutorial2_anomaly_detection.html" class="btn btn-neutral float-left" title="Tutorial 2.1: Intrusion Detection System" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../tutorial2_3_analyzing_application-layer_protocols/tutorial2_3_analyzing_application-layer_protocols.html" class="btn btn-neutral float-right" title="Tutorial 2.3: Analyzing Application-Layer Protocols" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Christoph R. Landolt, Mario Fritz.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>